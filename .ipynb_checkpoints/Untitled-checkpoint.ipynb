{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab5f649",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m box\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from ssl import AlertDescription\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os\n",
    "from html.parser import HTMLParser\n",
    "import decimal\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import box\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from itertools import repeat\n",
    "from functools import reduce\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP YOUR PATHS\n",
    "\n",
    "# names of the main directory containing folders named \"jsons\" and \"reports\"\n",
    "# Windows:\n",
    "# directory = r'D:\\Library RA\\dcat-metadata'\n",
    "# MAC or Linux:\n",
    "directory = r'.'\n",
    "\n",
    "\n",
    "# csv file contaning portal list \n",
    "portalFile = 'socrataPortals.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare list of metadata fields from the DCAT json schema for open data portals desired in the final report\n",
    "fieldnames = ['Title', 'Alternative Title', 'Description', 'Language', 'Creator', 'Title Source', 'Resource Class',\n",
    "              'Theme', 'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range', 'Spatial Coverage',\n",
    "              'Bounding Box', 'Resource Type', 'Format', 'Information', 'Download', 'ID', 'Identifier', 'Provider', 'Code', 'Member Of', 'Is Part Of', 'Rights',\n",
    "              'Accrual Method', 'Date Accessioned', 'Access Rights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do: remove the portal code stuff\n",
    "\n",
    "# list of fields to use for the portal status report\n",
    "statusFieldsReport = ['portalName', 'total', 'new_items', 'deleted_items']\n",
    "\n",
    "# dictionary using partial portal code to find out where the data portal belongs\n",
    "statedict = {'01': 'Indiana', '02': 'Illinois', '03': 'Iowa', '04': 'Maryland', '04c-01': 'District of Columbia', \n",
    "             '04f-01': '04f-01', '05': 'Minnesota', '06': 'Michigan', '07': 'Michigan', '08': 'Pennsylvania', \n",
    "             '09': 'Indiana', '10': 'Wisconsin', '11': 'Ohio', '12': 'Illinois', '13': 'Nebraska', '14': 'New Jersey', '99': 'Esri'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS\n",
    "\n",
    "# function to removes html tags from text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    return fieldvalue\n",
    "\n",
    "# function that prints metadata elements from the dictionary to a csv file (portal_status_report)\n",
    "# with as specified fields list as the header row.\n",
    "\n",
    "\n",
    "def printReport(report, dictionary, fields):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for keys in dictionary:\n",
    "            allvalues = dictionary[keys]\n",
    "            csvout.writerow(allvalues)\n",
    "\n",
    "# Similar to the function above but generates two csv files (allNewItems & allDeletedItems)\n",
    "\n",
    "\n",
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for portal in dictionary:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)\n",
    "\n",
    "# function that creates a dictionary with the position of a record in the data portal DCAT metadata json as the key\n",
    "# and the identifier as the value.\n",
    "\n",
    "\n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n",
    "\n",
    "\n",
    "def getTitles(data):\n",
    "    json_titles = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_titles[x] = data[\"dataset\"][x][\"title\"]\n",
    "    return json_titles\n",
    "\n",
    "'''Auto-generate Title field be like alternativeTitle [titleSource(place name)] {year if exist in alternative title}'''\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    # find if year exist in alternativeTitle\n",
    "    year = ''\n",
    "    try:  \n",
    "      year_range = re.findall(r'(\\d{4})-(\\d{4})', alternativeTitle)\n",
    "#     single_year = re.match(r'.*([1-3][0-9]{3})', alternativeTitle) \n",
    "    except:\n",
    "      year_range = ''\n",
    "    try: \n",
    "      single_year = re.match(r'.*(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})', alternativeTitle)\n",
    "    except:\n",
    "      single_year = ''    \n",
    "    if year_range:   # if a 'yyyy-yyyy' exists\n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  # or if a 'yyyy' exists\n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "     \n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'       \n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768efada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns a dictionary of selected metadata elements into a dictionary of new items (newItemDict) for each new item in a data portal.\n",
    "# This includes blank fields '' for columns that will be filled in manually later.\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    # y = position of the dataset in the DCAT metadata json, v = landing page URLs\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "\n",
    "#ALTERNATIVE TITLE\n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = cleanData(newdata[\"dataset\"][y]['title'])\n",
    "        except:\n",
    "            alternativeTitle = newdata[\"dataset\"][y]['title']\n",
    "            \n",
    "#DESCRIPTION\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        # Remove newline, whitespace, defalut description and replace singe quote, double quote\n",
    "        if description == \"{{default.description}}\":\n",
    "            description = description.replace(\"{{default.description}}\", \"\")\n",
    "        elif description == \"{{description}}\":\n",
    "            description = description.replace(\"{{description}}\", \"\")\n",
    "        else:\n",
    "            description = re.sub(r'[\\n]+|[\\r\\n]+', ' ',\n",
    "                                 description, flags=re.S)\n",
    "            description = re.sub(r'\\s{2,}', ' ', description)\n",
    "            description = description.replace(u\"\\u2019\", \"'\").replace(u\"\\u201c\", \"\\\"\").replace(u\"\\u201d\", \"\\\"\").replace(\n",
    "                u\"\\u00a0\", \"\").replace(u\"\\u00b7\", \"\").replace(u\"\\u2022\", \"\").replace(u\"\\u2013\", \"-\").replace(u\"\\u200b\", \"\")\n",
    "\n",
    " #CREATOR\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "                \n",
    "        theme = \"\"\n",
    "        try:\n",
    "            keyword = newdata[\"dataset\"][y][\"theme\"]\n",
    "            keyword_list = []\n",
    "            keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "        except:\n",
    "            keyword = \"\"\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "\n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "        \n",
    "        try:\n",
    "            rights = cleanData(newdata[\"dataset\"][y]['license'])\n",
    "        except:\n",
    "            rights = \"\"   \n",
    "\n",
    "# GET CLEAN IDENTIFIER\n",
    "        slug = 'socrata-' + identifier.rsplit('views/', 1)[-1]\n",
    "        querystring = parse_qs(urlparse(identifier).query)\n",
    "#         identifier_new = identifier\n",
    "\n",
    "        # auto-generate Title as alternativeTitle [titleSource] {YEAR if it exists in alternativeTitle}\n",
    "        title = format_title(alternativeTitle, titleSource)\n",
    "        # auto-generate Temporal Coverage and Date Range\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     # if title has {YYYY} or {YYYY-YYYY}\n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            dateRange = temporalCoverage[:4] + '-' + temporalCoverage[-4:]\n",
    "        else:\n",
    "            temporalCoverage = 'Continually updated resource'\n",
    "            \n",
    "#         resourceClass = \"\"\n",
    "#         resourceType = \"\"\n",
    "        # if 'LiDAR' exists in Title or Description, add it to Resource Type\n",
    "        if 'LiDAR' in title or 'LiDAR' in description:\n",
    "            resourceType = 'LiDAR'\n",
    "        if 'imagery' in title or 'imagery' in description or 'imagery' in keyword_list:\n",
    "            resourceClass = 'Imagery'\n",
    "        else:\n",
    "            resourceClass = 'Datasets'\n",
    "            \n",
    "                \n",
    "# DISTRIBUTION\n",
    "\n",
    "        format_types = []\n",
    "        resourceClass = \"\"\n",
    "        formatElement = \"\"\n",
    "        downloadURL = \"\"\n",
    "        resourceType = \"\"\n",
    "        webService = \"\"\n",
    "\n",
    "        try:\n",
    "              distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "              for dictionary in distribution:\n",
    "\n",
    "                      \n",
    "                      if dictionary[\"mediaType\"] == \"application/zip\":\n",
    "                          formatElement = \"Shapefile\"\n",
    "                          resourceType = \"Vector data\"\n",
    "                          downloadURL = dictionary[\"downloadURL\"] \n",
    "                          \n",
    "                      elif dictionary[\"mediaType\"] == \"application/vnd.google-earth.kml+xml\":\n",
    "                          formatElement = \"KML\"\n",
    "                          resourceType = \"Vector data\"\n",
    "\n",
    "                      elif dictionary[\"mediaType\"] == \"application/json\":\n",
    "                          formatElement = \"GeoJSON\"\n",
    "                          resourceType = \"Vector data\"\n",
    "                      \n",
    "                  # If the distribution section of the metadata is not structured in a typical way\n",
    "                      else:\n",
    "                         resourceClass = \"\"\n",
    "                         formatElement = \"\"\n",
    "        except:\n",
    "              continue\n",
    "                \n",
    "        metadataList = [title, alternativeTitle, description, language, creator, titleSource,\n",
    "                        resourceClass, theme, keyword_list, dateIssued, temporalCoverage,\n",
    "                        dateRange, spatialCoverage, bbox, resourceType,\n",
    "                        formatElement, information, downloadURL, slug, information, provider, portalName, memberOf, isPartOf, rights,\n",
    "                        accrualMethod, dateAccessioned, accessRights]\n",
    "\n",
    "        # deletes data portols except genere = 'Geospatial data' or 'Aerial imagery'\n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[6] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[slug] = metadata\n",
    "\n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96087bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_New_Items = []\n",
    "All_Deleted_Items = []\n",
    "Status_Report = {}\n",
    "\n",
    "# Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "ActionDate = time.strftime('%Y%m%d')\n",
    "\n",
    "# List all files in the 'jsons' folder under the current directory and store file names in the 'filenames' list\n",
    "filenames = os.listdir('jsons')\n",
    "\n",
    "# Open a list of portals and urls ending in /data.json from input CSV\n",
    "# using column headers 'portalName', 'URL', 'provider', 'SpatialCoverage'\n",
    "with open(portalFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Read in values from the portals list to be used within the script or as part of the metadata report\n",
    "        portalName = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        isPartOf  = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        bbox = row['Bounding Box']\n",
    "        print(portalName, url)\n",
    "        accrualMethod = \"Socrata\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        language = \"eng\"\n",
    "\n",
    "        # For each open data portal in the csv list...\n",
    "        # create an empty list to extract all previous action dates only from file names\n",
    "        dates = []\n",
    "\n",
    "        # loop over all file names in 'filenames' list and find the json files for the selected portal\n",
    "        # extract the previous action dates only from these files and store in the 'dates' list\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(portalName):\n",
    "                # format of filename is 'portalName_YYYYMMDD.json'\n",
    "                # 'YYYYMMDD' is located from index -13(included) to index -5(excluded)\n",
    "                dates.append(filename[-13:-5])\n",
    "\n",
    "        # remove action date from previous dates if any\n",
    "        # in case the script is run several times in one single day\n",
    "        # so the actionDate JSONs can overwrite those generated earlier on the same day \n",
    "        if ActionDate in dates:\n",
    "            dates.remove(ActionDate)\n",
    "\n",
    "        # find the latest action date from the 'dates' list\n",
    "        if dates:\n",
    "            PreviousActionDate = max(dates)\n",
    "        else:  # for brand new portals\n",
    "            PreviousActionDate='00000000'\n",
    "\n",
    "        # renames file paths based on portalName and manually provided dates\n",
    "        oldjson = directory + \\\n",
    "            '/jsons/%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = directory + '/jsons/%s_%s.json' % (portalName, ActionDate)\n",
    "\n",
    "        # if newjson already exists, do not need to request again\n",
    "        if os.path.exists(newjson):\n",
    "            with open(newjson, 'r') as fr:\n",
    "                newdata = json.load(fr)\n",
    "        else:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            # check if data portal URL is broken\n",
    "            if response.headers['content-type'] != 'application/json; charset=utf-8':\n",
    "                print(\"\\n--------------------- Data portal URL does not exist --------------------\\n\",\n",
    "                      portalName, url,  \"\\n--------------------------------------------------------------------------\\n\")\n",
    "                continue\n",
    "            else:\n",
    "                newdata = json.load(response)\n",
    "\n",
    "            # Saves a copy of the json to be used for the next round of comparison/reporting\n",
    "            with open(newjson, 'w', encoding='utf-8') as outfile:\n",
    "                json.dump(newdata, outfile)\n",
    "\n",
    "        # collects information about number of resources (total, new, and old) in each portal\n",
    "        status_metadata = []\n",
    "        status_metadata.append(portalName)\n",
    "\n",
    "        # Opens older copy of data/json downloaded from the specified Esri Open Data Portal.\n",
    "        # If this file does not exist, treats every item in the portal as new.\n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:\n",
    "                older_data = json.load(data_file)\n",
    "\n",
    "            # Makes a list of dataset identifiers in the older json\n",
    "            older_ids = getIdentifiers(older_data)\n",
    "            # UPDATE: Makes a list of dataset title in the older json\n",
    "            older_titles = getTitles(older_data)\n",
    "\n",
    "            # compares identifiers in the older json harvest of the data portal with identifiers in the new json,\n",
    "            # creating dictionaries with\n",
    "            # 1) a complete list of new json identifiers\n",
    "            # 2) a list of just the items that appear in the new json but not the older one\n",
    "            newjson_ids = {}\n",
    "            newitem_ids = {}\n",
    "\n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                newjson_ids[y] = identifier\n",
    "                if identifier not in older_ids.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "            \n",
    "            # UPDATE\n",
    "            newjson_titles = {}\n",
    "            newitem_ids = {}\n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                title = newdata[\"dataset\"][y][\"title\"]\n",
    "                newjson_titles[y] = title\n",
    "                if title not in older_titles.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "\n",
    "            # Creates a dictionary of metadata elements for each new data portal item.\n",
    "            # Includes an option to print a csv report of new items for each data portal.\n",
    "            # Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list\n",
    "            # (to be used printing the combined report)\n",
    "            # i.e. [portal1{identifier:[metadataElement1, metadataElement2, ... ],\n",
    "            # portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}]\n",
    "            All_New_Items.append(metadataNewItems(newdata, newitem_ids))\n",
    "\n",
    "#             # Compares identifiers in the older json to the list of identifiers from the newer json.\n",
    "#             # If the record no longer exists, adds selected fields into a dictionary of deleted items (deletedItemDict)\n",
    "#             deletedItemDict = {}\n",
    "\n",
    "\n",
    "            # collects information for the status report\n",
    "            status_metalist = [len(newjson_titles), len(\n",
    "                newitem_ids), len(deletedItemDict)]\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "\n",
    "        # if there is no older json for comparions....\n",
    "        else:\n",
    "            print(\"There is no comparison json for %s\" % (portalName))\n",
    "            # Makes a list of dataset identifiers in the new json\n",
    "            newjson_ids = getIdentifiers(newdata)\n",
    "\n",
    "            All_New_Items.append(metadataNewItems(newdata, newjson_ids))\n",
    "\n",
    "            # collects information for the status report\n",
    "            status_metalist = [len(newjson_ids), len(newjson_ids), '0']\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "\n",
    "        Status_Report[portalName] = status_metadata\n",
    "\n",
    "# prints two csv spreadsheets with all items that are new or deleted since the last time the data portals were harvested\n",
    "newItemsReport = directory + \\\n",
    "    \"/reports/allSocrataItems_%s.csv\" % (ActionDate)\n",
    "printItemReport(newItemsReport, fieldnames, All_New_Items)\n",
    "\n",
    "\n",
    "reportStatus = directory + \\\n",
    "    \"/reports/portal_status_report_%s.csv\" % (ActionDate)\n",
    "printReport(reportStatus, Status_Report, statusFieldsReport)\n",
    "\n",
    "\n",
    "# ---------- Populating Spatial Coverage -----------\n",
    "\n",
    "\"\"\" set file path \"\"\"\n",
    "# df_csv = pd.read_csv(newItemsReport, encoding='unicode_escape')\n",
    "df_csv = pd.read_csv(newItemsReport)\n",
    "\n",
    "\"\"\" split csv file if necessary \"\"\"\n",
    "# if records come from Esri, the spatial coverage is considered as United States\n",
    "# df_esri = df_csv[df_csv['Title Source'] == 'Esri'].reset_index(drop=True)\n",
    "# df_csv = df_csv[df_csv['Title Source'] != 'Esri'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\"\"\" split state from column 'Title Source' \"\"\"\n",
    "# -----------------------------------------\n",
    "# The portal code is the main indicator:\n",
    "# - 01 - Indiana\n",
    "# - 02 - Illinois\n",
    "# - 03 - Iowa\n",
    "# - 04 - Maryland\n",
    "# - 04c-01 - District of Columbia\n",
    "# - 04f-01 - Delaware, Philadelphia, Maryland, New Jersey\n",
    "# - 05 - Minnesota\n",
    "# - 06 - Michigan\n",
    "# - 07 - Michigan\n",
    "# - 08 - Pennsylvania\n",
    "# - 09 - Indiana\n",
    "# - 10 - Wisconsin\n",
    "# - 11 - Ohio\n",
    "# - 12 - Illinois\n",
    "# - 13 - Nebraska\n",
    "# - 99 - Esri\n",
    "# -----------------------------------------\n",
    "\n",
    "df_csv['State'] = [statedict[row['Code']] if row['Code'] in statedict.keys(\n",
    ") else statedict[row['Code'][0:2]] for _, row in df_csv.iterrows()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_newitems = pd.read_csv(newItemsReport)\n",
    "df_newitems.to_csv(newItemsReport, index=False)\n",
    "\n",
    "\n",
    "print(\"\\n--------------------- Congrats! ╰(￣▽￣)╯ --------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
