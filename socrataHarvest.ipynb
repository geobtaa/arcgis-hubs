{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9fb6d7",
   "metadata": {},
   "source": [
    " -*- coding: utf-8 -*-\n",
    " Need to define directory path (containing socrataPortals.csv, folder \"jsons\" and \"reports\"), and list of fields desired in the printed report\n",
    " The script currently prints one combined report of harvested items from all portals\n",
    " The script also prints a status report giving the total number of resources in the portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97998dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from ssl import AlertDescription\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os\n",
    "from html.parser import HTMLParser\n",
    "import decimal\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from itertools import repeat\n",
    "from functools import reduce\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a878605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP YOUR PATHS\n",
    "\n",
    "# names of the main directory containing folders named \"jsons\" and \"reports\"\n",
    "# Windows:\n",
    "# directory = r'D:\\Library RA\\dcat-metadata'\n",
    "# MAC or Linux:\n",
    "directory = r'.'\n",
    "\n",
    "\n",
    "# csv file contaning portal list \n",
    "portalFile = 'socrataPortals.csv'\n",
    "\n",
    "# list of metadata fields from the DCAT json schema for open data portals desired in the final report\n",
    "fieldnames = ['Title', 'Alternative Title', 'Description', 'Language', 'Creator', 'Title Source', 'Resource Class',\n",
    "              'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range', 'Spatial Coverage',\n",
    "              'Bounding Box', 'Resource Type', 'Format', 'Information', 'Download', 'ID', 'Identifier', 'Provider', 'Code', 'Member Of', 'Is Part Of', 'Rights',\n",
    "              'Accrual Method', 'Date Accessioned', 'Access Rights']\n",
    "\n",
    "# list of fields to use for the portal status report\n",
    "statusFieldsReport = ['portalName', 'total', 'new_items', 'deleted_items']\n",
    "\n",
    "# dictionary using partial portal code to find out where the data portal belongs\n",
    "statedict = {'01': 'Indiana', '02': 'Illinois', '03': 'Iowa', '04': 'Maryland', '04c-01': 'District of Columbia', \n",
    "             '04f-01': '04f-01', '05': 'Minnesota', '06': 'Michigan', '07': 'Michigan', '08': 'Pennsylvania', \n",
    "             '09': 'Indiana', '10': 'Wisconsin', '11': 'Ohio', '12': 'Illinois', '13': 'Nebraska', '14': 'New Jersey', '99': 'Esri'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b2ad34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to removes html tags from text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value):\n",
    "    fieldvalue = strip_tags(value)\n",
    "    return fieldvalue\n",
    "\n",
    "# function that prints metadata elements from the dictionary to a csv file (portal_status_report)\n",
    "# with as specified fields list as the header row.\n",
    "\n",
    "\n",
    "def printReport(report, dictionary, fields):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for keys in dictionary:\n",
    "            allvalues = dictionary[keys]\n",
    "            csvout.writerow(allvalues)\n",
    "\n",
    "# Similar to the function above but generates two csv files (allNewItems & allDeletedItems)\n",
    "\n",
    "\n",
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for portal in dictionary:\n",
    "            for keys in portal:\n",
    "                allvalues = portal[keys]\n",
    "                csvout.writerow(allvalues)\n",
    "\n",
    "# function that creates a dictionary with the position of a record in the data portal DCAT metadata json as the key\n",
    "# and the identifier as the value.\n",
    "\n",
    "\n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n",
    "\n",
    "\n",
    "def getTitles(data):\n",
    "    json_titles = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_titles[x] = data[\"dataset\"][x][\"title\"]\n",
    "    return json_titles\n",
    "\n",
    "'''Auto-generate Title field be like alternativeTitle [titleSource(place name)] {year if exist in alternative title}'''\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    # find if year exist in alternativeTitle\n",
    "    year = ''\n",
    "    try:  \n",
    "      year_range = re.findall(r'(\\d{4})-(\\d{4})', alternativeTitle)\n",
    "    except:\n",
    "      year_range = ''\n",
    "    try: \n",
    "      single_year = re.match(r'.*(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})', alternativeTitle)\n",
    "    except:\n",
    "      single_year = ''    \n",
    "    if year_range:   # if a 'yyyy-yyyy' exists\n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  # or if a 'yyyy' exists\n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "     \n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'       \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5dcc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns a dictionary of selected metadata elements into a dictionary of new items (newItemDict) for each new item in a data portal.\n",
    "# This includes blank fields '' for columns that will be filled in manually later.\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    # y = position of the dataset in the DCAT metadata json, v = landing page URLs\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "\n",
    "#ALTERNATIVE TITLE\n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = cleanData(newdata[\"dataset\"][y]['title'])\n",
    "        except:\n",
    "            alternativeTitle = newdata[\"dataset\"][y]['title']\n",
    "\n",
    "    #DESCRIPTION\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        # Remove newline, whitespace, defalut description and replace singe quote, double quote\n",
    "        if description == \"{{default.description}}\":\n",
    "            description = description.replace(\"{{default.description}}\", \"\")\n",
    "        elif description == \"{{description}}\":\n",
    "            description = description.replace(\"{{description}}\", \"\")\n",
    "        else:\n",
    "            description = re.sub(r'[\\n]+|[\\r\\n]+', ' ',\n",
    "                                 description, flags=re.S)\n",
    "            description = re.sub(r'\\s{2,}', ' ', description)\n",
    "            description = description.replace(u\"\\u2019\", \"'\").replace(u\"\\u201c\", \"\\\"\").replace(u\"\\u201d\", \"\\\"\").replace(\n",
    "                u\"\\u00a0\", \"\").replace(u\"\\u00b7\", \"\").replace(u\"\\u2022\", \"\").replace(u\"\\u2013\", \"-\").replace(u\"\\u200b\", \"\")\n",
    "\n",
    "     #CREATOR\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "\n",
    "        theme = \"\"\n",
    "        try:\n",
    "            keyword = newdata[\"dataset\"][y][\"theme\"]\n",
    "            keyword_list = []\n",
    "            keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "        except:\n",
    "            keyword = \"\"\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "\n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "\n",
    "        try:\n",
    "            rights = cleanData(newdata[\"dataset\"][y]['license'])\n",
    "        except:\n",
    "            rights = \"\"  \n",
    "            \n",
    "    # GET CLEAN IDENTIFIER\n",
    "        slug = 'socrata-' + identifier.rsplit('views/', 1)[-1]\n",
    "        querystring = parse_qs(urlparse(identifier).query)\n",
    "    #         identifier_new = identifier\n",
    "\n",
    "        # auto-generate Title as alternativeTitle [titleSource] {YEAR if it exists in alternativeTitle}\n",
    "        title = format_title(alternativeTitle, titleSource)\n",
    "        # auto-generate Temporal Coverage and Date Range\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     # if title has {YYYY} or {YYYY-YYYY}\n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            dateRange = temporalCoverage[:4] + '-' + temporalCoverage[-4:]\n",
    "        else:\n",
    "            temporalCoverage = 'Continually updated resource'\n",
    "\n",
    "\n",
    "#         # if 'LiDAR' exists in Title or Description, add it to Resource Type\n",
    "#         if 'LiDAR' in title or 'LiDAR' in description:\n",
    "#             resourceType = 'LiDAR'\n",
    "#         if 'imagery' in title or 'imagery' in description or 'imagery' in keyword_list:\n",
    "#             resourceClass = 'Imagery'\n",
    "#         else:\n",
    "#             resourceClass = 'Datasets'\n",
    "                \n",
    "\n",
    "    # DISTRIBUTION\n",
    "\n",
    "        format_types = []\n",
    "        formatElement = \"\"\n",
    "        downloadURL = \"\"\n",
    "        resourceType = \"\"\n",
    "\n",
    "\n",
    "        # Only fills metadata for Shapefile downloads\n",
    "        \n",
    "        try:\n",
    "            distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "            for dictionary in distribution:\n",
    "                media_type = dictionary[\"mediaType\"]\n",
    "\n",
    "                if media_type == \"application/zip\":\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    resourceType = \"Vector data\"\n",
    "                    downloadURL = dictionary[\"downloadURL\"]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        except:\n",
    "            pass  # Handle exceptions (e.g. if \"distribution\" or \"mediaType\" keys are missing)\n",
    "\n",
    "\n",
    "\n",
    "    ######## Create the list of metadata ###############\n",
    "\n",
    "        metadataList = [title, alternativeTitle, description, language, creator, titleSource,\n",
    "                        resourceClass, keyword_list, dateIssued, temporalCoverage,\n",
    "                        dateRange, spatialCoverage, bbox, resourceType,\n",
    "                        formatElement, information, downloadURL, slug, information, provider, portalName, memberOf, isPartOf, rights,\n",
    "                        accrualMethod, dateAccessioned, accessRights]\n",
    "\n",
    "        # does not include datasets where Format (the 15th column) is empty\n",
    "        for i in range(len(metadataList)):\n",
    "                if metadataList[14] != \"\":\n",
    "                    metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[slug] = metadata\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64dfc011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02b-17117 https://data.macoupincountyil.gov/data.json\n",
      "There is no comparison json for 02b-17117\n",
      "04b-24027 https://opendata.howardcountymd.gov/data.json\n",
      "There is no comparison json for 04b-24027\n",
      "04b-24033 https://data.princegeorgescountymd.gov/data.json\n",
      "There is no comparison json for 04b-24033\n",
      "11c-01 https://data.cincinnati-oh.gov/data.json\n",
      "There is no comparison json for 11c-01\n",
      "12b-17031-2 https://datacatalog.cookcountyil.gov/data.json\n",
      "There is no comparison json for 12b-17031-2\n",
      "12c-01 https://data.cityofchicago.org/data.json\n",
      "There is no comparison json for 12c-01\n",
      "\n",
      "--------------------- Congrats! ╰(￣▽￣)╯ --------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_New_Items = []\n",
    "# All_Deleted_Items = []\n",
    "Status_Report = {}\n",
    "\n",
    "# Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "ActionDate = time.strftime('%Y%m%d')\n",
    "\n",
    "# List all files in the 'jsons' folder under the current directory and store file names in the 'filenames' list\n",
    "filenames = os.listdir('jsons')\n",
    "\n",
    "# Open a list of portals and urls ending in /data.json from input CSV\n",
    "# using column headers 'portalName', 'URL', 'provider', 'SpatialCoverage'\n",
    "with open(portalFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Read in values from the portals list to be used within the script or as part of the metadata report\n",
    "        portalName = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        isPartOf  = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        bbox = row['Bounding Box']\n",
    "        print(portalName, url)\n",
    "        resourceClass = \"Datasets\"\n",
    "        accrualMethod = \"Socrata\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        language = \"eng\"\n",
    "\n",
    "        # For each open data portal in the csv list...\n",
    "        # create an empty list to extract all previous action dates only from file names\n",
    "        dates = []\n",
    "\n",
    "        # loop over all file names in 'filenames' list and find the json files for the selected portal\n",
    "        # extract the previous action dates only from these files and store in the 'dates' list\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(portalName):\n",
    "                # format of filename is 'portalName_YYYYMMDD.json'\n",
    "                # 'YYYYMMDD' is located from index -13(included) to index -5(excluded)\n",
    "                dates.append(filename[-13:-5])\n",
    "\n",
    "        # remove action date from previous dates if any\n",
    "        # in case the script is run several times in one single day\n",
    "        # so the actionDate JSONs can overwrite those generated earlier on the same day \n",
    "        if ActionDate in dates:\n",
    "            dates.remove(ActionDate)\n",
    "\n",
    "        # find the latest action date from the 'dates' list\n",
    "        if dates:\n",
    "            PreviousActionDate = max(dates)\n",
    "        else:  # for brand new portals\n",
    "            PreviousActionDate='00000000'\n",
    "\n",
    "        # renames file paths based on portalName and manually provided dates\n",
    "        oldjson = directory + \\\n",
    "            '/jsons/%s_%s.json' % (portalName, PreviousActionDate)\n",
    "        newjson = directory + '/jsons/%s_%s.json' % (portalName, ActionDate)\n",
    "\n",
    "        # if newjson already exists, do not need to request again\n",
    "        if os.path.exists(newjson):\n",
    "            with open(newjson, 'r') as fr:\n",
    "                newdata = json.load(fr)\n",
    "        else:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            # check if data portal URL is broken\n",
    "            if response.headers['content-type'] != 'application/json; charset=utf-8':\n",
    "                print(\"\\n--------------------- Data portal URL does not exist --------------------\\n\",\n",
    "                      portalName, url,  \"\\n--------------------------------------------------------------------------\\n\")\n",
    "                continue\n",
    "            else:\n",
    "                newdata = json.load(response)\n",
    "\n",
    "            # Saves a copy of the json to be used for the next round of comparison/reporting\n",
    "            with open(newjson, 'w', encoding='utf-8') as outfile:\n",
    "                json.dump(newdata, outfile)\n",
    "\n",
    "        # collects information about number of resources (total, new, and old) in each portal\n",
    "        status_metadata = []\n",
    "        status_metadata.append(portalName)\n",
    "\n",
    "        # Opens older copy of data/json downloaded from the specified Esri Open Data Portal.\n",
    "        # If this file does not exist, treats every item in the portal as new.\n",
    "        if os.path.exists(oldjson):\n",
    "            with open(oldjson) as data_file:\n",
    "                older_data = json.load(data_file)\n",
    "\n",
    "            # Makes a list of dataset identifiers in the older json\n",
    "            older_ids = getIdentifiers(older_data)\n",
    "            # UPDATE: Makes a list of dataset title in the older json\n",
    "            older_titles = getTitles(older_data)\n",
    "\n",
    "            # compares identifiers in the older json harvest of the data portal with identifiers in the new json,\n",
    "            # creating dictionaries with\n",
    "            # 1) a complete list of new json identifiers\n",
    "            # 2) a list of just the items that appear in the new json but not the older one\n",
    "            newjson_ids = {}\n",
    "            newitem_ids = {}\n",
    "\n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                newjson_ids[y] = identifier\n",
    "                if identifier not in older_ids.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "\n",
    "            # UPDATE\n",
    "            newjson_titles = {}\n",
    "            newitem_ids = {}\n",
    "            for y in range(len(newdata[\"dataset\"])):\n",
    "                identifier = newdata[\"dataset\"][y][\"identifier\"]\n",
    "                title = newdata[\"dataset\"][y][\"title\"]\n",
    "                newjson_titles[y] = title\n",
    "                if title not in older_titles.values():\n",
    "                    newitem_ids[y] = identifier\n",
    "\n",
    "            # Creates a dictionary of metadata elements for each new data portal item.\n",
    "            # Includes an option to print a csv report of new items for each data portal.\n",
    "            # Puts dictionary of identifiers (key), metadata elements (values) for each data portal into a list\n",
    "            # (to be used printing the combined report)\n",
    "            # i.e. [portal1{identifier:[metadataElement1, metadataElement2, ... ],\n",
    "            # portal2{identifier:[metadataElement1, metadataElement2, ... ], ...}]\n",
    "            All_New_Items.append(metadataNewItems(newdata, newitem_ids))\n",
    "\n",
    "    #             # Compares identifiers in the older json to the list of identifiers from the newer json.\n",
    "    #             # If the record no longer exists, adds selected fields into a dictionary of deleted items (deletedItemDict)\n",
    "    #             deletedItemDict = {}\n",
    "\n",
    "\n",
    "            # collects information for the status report\n",
    "            status_metalist = [len(newjson_titles), len(\n",
    "                newitem_ids), len(deletedItemDict)]\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "\n",
    "        # if there is no older json for comparions....\n",
    "        else:\n",
    "            print(\"There is no comparison json for %s\" % (portalName))\n",
    "            # Makes a list of dataset identifiers in the new json\n",
    "            newjson_ids = getIdentifiers(newdata)\n",
    "\n",
    "            All_New_Items.append(metadataNewItems(newdata, newjson_ids))\n",
    "\n",
    "            # collects information for the status report\n",
    "            status_metalist = [len(newjson_ids), len(newjson_ids), '0']\n",
    "            for value in status_metalist:\n",
    "                status_metadata.append(value)\n",
    "\n",
    "        Status_Report[portalName] = status_metadata\n",
    "\n",
    "# prints two csv spreadsheets with all items that are new or deleted since the last time the data portals were harvested\n",
    "newItemsReport = directory + \\\n",
    "    \"/reports/allSocrataItems_%s.csv\" % (ActionDate)\n",
    "printItemReport(newItemsReport, fieldnames, All_New_Items)\n",
    "\n",
    "\n",
    "reportStatus = directory + \\\n",
    "    \"/reports/socrata-portalReport_%s.csv\" % (ActionDate)\n",
    "printReport(reportStatus, Status_Report, statusFieldsReport)\n",
    "\n",
    "\n",
    "# ---------- Populating Spatial Coverage -----------\n",
    "\n",
    "\"\"\" set file path \"\"\"\n",
    "# df_csv = pd.read_csv(newItemsReport, encoding='unicode_escape')\n",
    "df_csv = pd.read_csv(newItemsReport)\n",
    "\n",
    "\"\"\" split csv file if necessary \"\"\"\n",
    "# if records come from Esri, the spatial coverage is considered as United States\n",
    "# df_esri = df_csv[df_csv['Title Source'] == 'Esri'].reset_index(drop=True)\n",
    "# df_csv = df_csv[df_csv['Title Source'] != 'Esri'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\"\"\" split state from column 'Title Source' \"\"\"\n",
    "# -----------------------------------------\n",
    "# The portal code is the main indicator:\n",
    "# - 01 - Indiana\n",
    "# - 02 - Illinois\n",
    "# - 03 - Iowa\n",
    "# - 04 - Maryland\n",
    "# - 04c-01 - District of Columbia\n",
    "# - 04f-01 - Delaware, Philadelphia, Maryland, New Jersey\n",
    "# - 05 - Minnesota\n",
    "# - 06 - Michigan\n",
    "# - 07 - Michigan\n",
    "# - 08 - Pennsylvania\n",
    "# - 09 - Indiana\n",
    "# - 10 - Wisconsin\n",
    "# - 11 - Ohio\n",
    "# - 12 - Illinois\n",
    "# - 13 - Nebraska\n",
    "# - 99 - Esri\n",
    "# -----------------------------------------\n",
    "\n",
    "df_csv['State'] = [statedict[row['Code']] if row['Code'] in statedict.keys(\n",
    ") else statedict[row['Code'][0:2]] for _, row in df_csv.iterrows()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_newitems = pd.read_csv(newItemsReport)\n",
    "df_newitems.to_csv(newItemsReport, index=False)\n",
    "\n",
    "\n",
    "print(\"\\n--------------------- Congrats! ╰(￣▽￣)╯ --------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
